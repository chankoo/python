{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n-gram\n",
    "# n-gram 은 n개 어절/음절을 연쇄적으로 분류해 그 빈도를 따진다\n",
    "# n=1 일 때는 unigram, 2일 때는 bigram, 3 - trigram \n",
    "# 그리고 긴 텍스트를 분석하게 된다면 같은 n-gram 이 여러개 나온다 \n",
    "# n-gram 들의 수를 세서 리스트로 만든 것을 빈도 리스트 (frequency list) 라고 한다 \n",
    "# 이상의 날개에서 가장 많이 나온 n-gram 이 뭘까 궁금해서 Python으로 n-gram 분석을 해봤습니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nalgae.txt', 'r', encoding='utf-8', newline='\\n') as f:\n",
    "    file = f.read()\n",
    "sample_file = ''.join(file)[6:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"박제(剝製)가 되어 버린 천재'를 아시오? 나는 유쾌하오. 이런 때 연애까지가 유쾌하오.\\r\\n육신이 흐느적흐느적하도록 피로했을 때만 정신이 은화처럼 맑소. 니코틴이 내 횟배 앓는 뱃속\\r\\n으로 스미면 머릿속에 으레 백지가 준비되는 법이오. 그 위에다 나는 위트와 파라독스를 바둑 포\\r\\n석처럼 늘어 놓소. 가공할 상식의 병이오.\\r\\n나는 또 여인과 생활을 설계하오. 연애기법에마저 서먹서먹해진 지성의 극치를 흘깃 좀 들여다\\r\\n본 일이 있는, 말하자면 일종의 정신분일자(정신이 제멋대로 노는 사람)말이오. 이런 여인의 반\\r\\n----그것은 온갖 것\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#어절 n-gram 분석\n",
    "#sentence: 분석할 문장, num_gram: n-gram 단위\n",
    "def word_ngram(sentence, num_gram):\n",
    "    # in the case a file is given, remove escape characters\n",
    "    # sentence = sentence.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    sentence = sentence.replace('\\n', ' ').replace('\\r', ' ').replace('  ',' ')\n",
    "    text = tuple(sentence.split(' '))\n",
    "    ngrams = [text[x:x+num_gram] for x in range(0, len(text))]\n",
    "    return tuple(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('박제(剝製)가', '되어'),\n",
       " ('되어', '버린'),\n",
       " ('버린', \"천재'를\"),\n",
       " (\"천재'를\", '아시오?'),\n",
       " ('아시오?', '나는'),\n",
       " ('나는', '유쾌하오.'),\n",
       " ('유쾌하오.', '이런'),\n",
       " ('이런', '때'),\n",
       " ('때', '연애까지가'),\n",
       " ('연애까지가', '유쾌하오.'),\n",
       " ('유쾌하오.', '육신이'),\n",
       " ('육신이', '흐느적흐느적하도록'),\n",
       " ('흐느적흐느적하도록', '피로했을'),\n",
       " ('피로했을', '때만'),\n",
       " ('때만', '정신이'),\n",
       " ('정신이', '은화처럼'),\n",
       " ('은화처럼', '맑소.'),\n",
       " ('맑소.', '니코틴이'),\n",
       " ('니코틴이', '내'),\n",
       " ('내', '횟배'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ngram(sample_file,2)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음절 n-gram 분석\n",
    "#sentence: 분석할 문장, num_gram: n-gram 단위\n",
    "def phoneme_ngram(sentence, num_gram):\n",
    "    text = tuple(sentence) # split the sentence into an array of characters\n",
    "    ngrams = [text[x:x+num_gram] for x in range(0, len(text))]\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('박', '제', '('),\n",
       " ('제', '(', '剝'),\n",
       " ('(', '剝', '製'),\n",
       " ('剝', '製', ')'),\n",
       " ('製', ')', '가'),\n",
       " (')', '가', ' '),\n",
       " ('가', ' ', '되'),\n",
       " (' ', '되', '어'),\n",
       " ('되', '어', ' '),\n",
       " ('어', ' ', '버')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_ngram(sample_file,3)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n-gram 빈도 리스트 생성\n",
    "def make_freqlist(ngrams):\n",
    "    freqlist = {}\n",
    " \n",
    "    for ngram in ngrams:\n",
    "        if (ngram in freqlist):\n",
    "            freqlist[ngram] += 1\n",
    "        else:\n",
    "            freqlist[ngram] = 1\n",
    "    return freqlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = word_ngram(sample_file, 3)\n",
    "freqlist = make_freqlist(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('알', '수', '없다.'), 4), (('그', '돈', '오'), 4), (('돈을', '놓고', '가는'), 3), (('것', '같았다.', '나는'), 3), (('오', '원', '돈을'), 3), (('해가', '드는', '것을'), 2), (('수', '없다.', '나는'), 2), (('나는', '내', '아내'), 2), (('내', '아내', '외의'), 2), (('아내', '외의', '다른'), 2)]\n"
     ]
    }
   ],
   "source": [
    "#sorted_freqlist = sorted(freqlist.items(), key=operator.itemgetter(1))\n",
    "sorted_freqlist = sorted(freqlist.items(), key=lambda elem: elem[1],reverse=True)\n",
    "print(sorted_freqlist[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률론적 언어 모형\n",
    "확률론적 언어 모형(Probabilistic Language Model)은  mm 개의 단어  w1,w2,…,wmw1,w2,…,wm열(word sequence)이 \n",
    "주어졌을 때 문장으로써 성립될 확률  P(w1,w2,…,wm)P(w1,w2,…,wm)  을 출력함으로써 이 단어 열이 실제로 현실에서 \n",
    "사용될 수 있는 문장(sentence)인지를 판별하는 모형이다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__유니그램 모형 (Unigram Model)¶__\n",
    "\n",
    "만약 모든 단어의 활용이 완전히 서로 독립이라면 단어 열의 확률은 다음과 같이 각 단어의 확률의 곱이 된다. 이러한 모형을 유니그램 모형 (Unigram Model)이라고 한다.\n",
    "\n",
    "$P(w_1,w_2,…,w_m)=\\prod_{i=1}^m P(w_i)$\n",
    " \n",
    "__바이그램 모형 (Bigram Model)¶__\n",
    "\n",
    "만약 단어의 활용이 바로 전 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 Bigram 모형 또는 마코프 모형(Markov Model)이라고 한다.\n",
    "\n",
    "$P(w_1,w_2,…,w_m)=P(w_1)\\prod_{i=2}^m P(w_i|w_{i−1})$\n",
    " \n",
    "__N-그램 모형 (N-gram Model)¶__\n",
    "\n",
    "만약 단어의 활용이 바로 전  n 개의 단어에만 의존한다면 단어 열의 확률은 다음과 같다. 이러한 모형을 N-gram 모형이라고 한다.\n",
    "$P(w_1,w_2,…,w_m)=P(w_1)\\prod_{i=n}^m P(w_i|w_{i−1},…,w_{i−n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률 추정 방법\n",
    "실제 텍스트 코퍼스(corpus)에서 확률을 추정하는 방법은 다음과 같다. 여기에서는 바이그램의 경우를 살펴본다.\n",
    "\n",
    "일단 모든 문장에 문장의 시작과 끝을 나타내는 특별 토큰을 추가한다. \n",
    "예를 들어 문장의 시작은 SS, 문장의 끝은 SE 이라는 토큰을 사용할 수 있다.\n",
    "\n",
    "바이그램 모형에서는 전체 문장의 확률은 다음과 같이 조건부 확률의 곱으로 나타난다.\n",
    "\n",
    "P(SS I am a boy SE)=P(I|SS)⋅P(am|I)⋅P(a|am)⋅P(boy|a)⋅P(SE|boy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예제\n",
    "다음은 nltk 패키지의 샘플 코퍼스인 movie_reviews의 텍스트를 기반으로 N-그램 모형을 추정하고 모형 확률로부터 \n",
    "랜덤하게 문장을 생성하는 예제이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# 문서를 문장으로 분리\n",
    "sentences = list(movie_reviews.sents())\n",
    "\n",
    "import random\n",
    "# 섞는다.\n",
    "random.seed(180607)\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wayne', \"'\", 's', 'greatest', 'ambition', 'is', 'to', 'become', 'a', 'night']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 이 입력으로부터 확률값을 추정한다.\n",
    "import collections, math\n",
    "from math import log\n",
    "from collections import Counter\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "\n",
    "def stringify_context(context):\n",
    "    return(\" \".join(context))\n",
    "\n",
    "\n",
    "boundaryToken = \"\"\n",
    "def ngrams(n, sentences, boundaryToken=boundaryToken, verbose=False):\n",
    "    c = {}\n",
    "    q = []\n",
    "    for i in range(n-1):\n",
    "        q.append(boundaryToken)\n",
    "    for sentence in sentences:\n",
    "        for w in sentence + [boundaryToken]:\n",
    "            context_gram = stringify_context(q)\n",
    "            if verbose:\n",
    "                print(q)\n",
    "                print(context_gram)\n",
    "                print(w)\n",
    "            if not context_gram in c:\n",
    "                c[context_gram] = Counter()\n",
    "            c[context_gram][w] += 1\n",
    "            q.pop(0)\n",
    "            q.append(w)\n",
    "    return(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'\": 7,\n",
       "         'are': 4,\n",
       "         'can': 3,\n",
       "         'do': 1,\n",
       "         'don': 1,\n",
       "         'feel': 1,\n",
       "         'focus': 1,\n",
       "         'get': 2,\n",
       "         'have': 2,\n",
       "         'join': 1,\n",
       "         'just': 1,\n",
       "         'know': 2,\n",
       "         'later': 1,\n",
       "         'learn': 1,\n",
       "         'like': 1,\n",
       "         'might': 1,\n",
       "         'never': 1,\n",
       "         'should': 1,\n",
       "         'were': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams(2, sentences[:1000])[\"we\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BigramModel:\n",
    "    \n",
    "    def __init__(self, training_sentences, smoothing='none'):\n",
    "        train = ngrams(2, training_sentences)\n",
    "        self.probs = {}\n",
    "        if smoothing == 'none':\n",
    "            for context_gram in train.keys():\n",
    "                N = sum(train[context_gram].values())\n",
    "                self.probs[context_gram] = Counter({k:v/N for k,v in train[context_gram].items()})\n",
    "\n",
    "    def prob(self, word, context):\n",
    "        \"\"\"takes a word string and a context which is a list of word strings, and returns the probability of the word\"\"\"\n",
    "        c = stringify_context(context)\n",
    "        return(self.probs[c][word])\n",
    "\n",
    "    def scoreSentence(self, sentence, verbose=False):\n",
    "        context = [boundaryToken]\n",
    "        result = 0\n",
    "        for w in sentence + [boundaryToken]:\n",
    "            lp = log(self.prob(w, context))\n",
    "            result = result + lp\n",
    "            if verbose:\n",
    "                pprint([context, w, lp])\n",
    "            context = [w]\n",
    "        return result\n",
    "\n",
    "    def generateSentence(self, verbose=False, goryDetails=False):\n",
    "        context = [boundaryToken]\n",
    "        result = []\n",
    "        w = None\n",
    "        while not w == boundaryToken:\n",
    "            r = random.random() # returns a random float between 0 and 1\n",
    "            x = 0\n",
    "            c = self.probs[stringify_context(context)] # this will be a Counter\n",
    "            w = c.keys()[np.argmax(np.random.multinomial(1, c.values(), (1,))[0])]\n",
    "            result.append(w)\n",
    "            context = [w]\n",
    "            if verbose:\n",
    "                print(w)\n",
    "        result.pop() # drop the boundary token\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BigramModel(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018562267971650354"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 트레이닝이 끝나면 조건부 확률의 값을 보거나 샘플 문장을 입력해서 문장의 로그 확률을 구할 수 있다.\n",
    "\n",
    "# \"i\" 라는 단어가 나온 뒤에 \"am\"이라는 단어가 나올 확률을 계산하면\n",
    "m.prob(\"am\", [\"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624749529418908"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.prob(\"\", [\".\"])  # .(마침표) 뒤에 문장이 끝날 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26083768673815416\n",
      "0.0001437363613793464\n"
     ]
    }
   ],
   "source": [
    "print(m.prob(\"the\", [\"in\"]))  # in 뒤에 the 가 올 확률\n",
    "print(m.prob(\"in\", [\"the\"]))  # the 뒤에 in 이 올 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"'\": 0.03571428571428571,\n",
       "         ',': 0.17857142857142858,\n",
       "         '-': 0.07142857142857142,\n",
       "         '.': 0.10714285714285714,\n",
       "         'cider': 0.03571428571428571,\n",
       "         'computer': 0.03571428571428571,\n",
       "         'hasn': 0.03571428571428571,\n",
       "         'in': 0.03571428571428571,\n",
       "         'orchard': 0.03571428571428571,\n",
       "         'picker': 0.03571428571428571,\n",
       "         'pickers': 0.07142857142857142,\n",
       "         'pie': 0.2857142857142857,\n",
       "         'that': 0.03571428571428571})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.probs[\"apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], 'in', -3.7639298908174825]\n",
      "[['in'], 'the', -1.343856955005301]\n",
      "[['the'], '1970s', -9.45366556371934]\n",
      "[['1970s'], '.', -1.413693335308005]\n",
      "[['.'], '', -0.038247236076315826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-16.013392980926444"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = ['in', 'the', '1970s', '.']\n",
    "m.scoreSentence(test_sentence, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[''], 'i', -3.3655219750193166]\n",
      "[['i'], 'am', -3.9866243623410944]\n",
      "[['am'], 'a', -2.6441463991227296]\n",
      "[['a'], 'boy', -7.370073198683084]\n",
      "[['boy'], '.', -2.4904468301636156]\n",
      "[['.'], '', -0.038247236076315826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-19.895060001406158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.scoreSentence([\"i\", \"am\", \"a\", \"boy\", \".\"], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확률론적 언어 모형의 활용\n",
    "확률론적 언어 모형은 다음과 같은 분야에 광범위하게 활용할 수 있다.\n",
    "\n",
    "철자 및 문법 교정(Spell Correction)\n",
    "음성 인식(Speech Recognition)\n",
    "자동 번역(Machine Translation)\n",
    "자동 요약(Summarization)\n",
    "챗봇(Question-Answering)\n",
    "\n",
    "https://datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
