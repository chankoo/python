{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizaition Methods\n",
    "![7](https://user-images.githubusercontent.com/38183218/44017435-440dd208-9f13-11e8-8d7c-0fc626274bc8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent(full batch와 비교)\n",
    "- 배치마다 gradient 내려가는 모양다르기에 local minima에 빠지는 위험 피할수있음\n",
    "![1](https://user-images.githubusercontent.com/38183218/44017423-41111d30-9f13-11e8-9181-b582c4973096.PNG)\n",
    "![1 5](https://user-images.githubusercontent.com/38183218/44017424-413a8de6-9f13-11e8-9117-65d2c48b8d4c.PNG)\n",
    "![2](https://user-images.githubusercontent.com/38183218/44017425-41632972-9f13-11e8-861d-fc9ad89036b7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum Update\n",
    "![3](https://user-images.githubusercontent.com/38183218/44017426-418b4b64-9f13-11e8-92cb-3ec0ca5b61b7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad Update\n",
    "![4](https://user-images.githubusercontent.com/38183218/44017432-439170aa-9f13-11e8-869d-bddbaea10c9b.PNG)\n",
    "노드마다 lr다르게 걸어서 학습률 최적화\n",
    "\n",
    "cache에 \n",
    "    ----------------\n",
    "cache값이 점점 증가하므로 어느순간 dx가 0 되는 단점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSProp Update\n",
    "![5](https://user-images.githubusercontent.com/38183218/44017433-43bab082-9f13-11e8-9fb1-ad410728df20.PNG)\n",
    "moving average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Update\n",
    "![6](https://user-images.githubusercontent.com/38183218/44017434-43e42aca-9f13-11e8-940f-d0b05f0abb55.PNG) \n",
    "오타 m/(np.sqrt\n",
    "모멘텀 + RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "레이어가 쌓일 수록 이상치인 batch가 가져오는 오차는 커진다\n",
    "\n",
    "그래서 애초에 zero-centered로 normalize해버리면 어떠냐\n",
    "\n",
    ": 각 층이 활성화를 적당히 퍼뜨리도록 '강제' 하는 것\n",
    "- 학습속도개선\n",
    "- 초기값에 크게 의존x\n",
    "- 오버피팅 억제해 드롭아웃 등의 필요성 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "#### Ensembles\n",
    "#### Add term to loss\n",
    "#### Dropout\n",
    "어떤 feature의 가중치가 너무 커지면 drop하여 오버피팅 문제해결\n",
    "#### Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
