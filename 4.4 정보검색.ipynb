{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정보검색\n",
    "1. Vector Space Model\n",
    "2. Indexing\n",
    "3. Inverted Document\n",
    "4. Weighting\n",
    "5. Retrieving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF/IDF\n",
    "![7](https://user-images.githubusercontent.com/38183218/43502447-e3624b86-9595-11e8-895b-150ed944052c.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarity\n",
    "![8](https://user-images.githubusercontent.com/38183218/43502459-f96733f6-9595-11e8-8094-60a1791d8daa.jpg)\n",
    "![9](https://user-images.githubusercontent.com/38183218/43502460-f991e916-9595-11e8-880c-6295e33fdf91.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실습: 뉴스 검색 엔진을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "from math import log10, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(\"http://media.daum.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.content,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aList = soup.select('.list_headline  a.link_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for a in aList:\n",
    "    links.append(a.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://v.media.daum.net/v/20180801141138247',\n",
       " 'http://v.media.daum.net/v/20180801140951174',\n",
       " 'http://v.media.daum.net/v/20180801140921160',\n",
       " 'http://v.media.daum.net/v/20180801140802122',\n",
       " 'http://v.media.daum.net/v/20180801140622059',\n",
       " 'http://v.media.daum.net/v/20180801140406973',\n",
       " 'http://v.media.daum.net/v/20180801140352961',\n",
       " 'http://v.media.daum.net/v/20180801140009800',\n",
       " 'http://v.media.daum.net/v/20180801140007797',\n",
       " 'http://v.media.daum.net/v/20180801140007793',\n",
       " 'http://v.media.daum.net/v/20180801140004784',\n",
       " 'http://v.media.daum.net/v/20180801135922758']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_docs = []\n",
    "for link in links:\n",
    "    resp = requests.get(link)\n",
    "    news_soup = BeautifulSoup(resp.content,'lxml')\n",
    "    \n",
    "    title = news_soup.find('title')\n",
    "    pars = news_soup.select('.article_view p ')\n",
    "    article = ''.join([par.text for par in pars if len(par)>0])\n",
    "    news_docs.append((title.text, article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('강원도 홍천 40.3도..우리나라 기상관측 이래 역대 최고(2보) | Daum 뉴스',\n",
       " '(서울=연합뉴스) 김승욱 기자 = 강원도 홍천의 수은주가 1일 40.3도까지 치솟아 기상관측 이래 역대 최고기온을 갈아치웠다.기상청에 따르면 이날 오후 1시 59시께 40.1도를 기록한 뒤 오후 2시 1분께 40.3도로 기온이 더 올랐다.우리나라 기상관측 역대 최고 온도다.부산·인천 1904년, 서울 1907년 등 현대적인 기상관측 장비가 도입된 20세기 초반 이래 전국에서 40도를 돌파한 적은 1942년 8월 1일 대구(40.0도)가 유일했다.앞서 경북 의성은 올해 7월 27일 39.9도, 충북 추풍령은 1939년 7월 21일 39.8도를 기록한 바 있다.ksw08@yna.co.kr')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_docs)):\n",
    "    with open('docs/news_docs{0}.txt'.format(i),'w',encoding='utf-8') as fp:\n",
    "        fp.write(news_docs[i][0]+'\\n'+news_docs[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = glob.glob('/Users/Chankoo/Desktop/GitHub/python/docs/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "강원도\n",
      "['강원', '원도']\n",
      "홍천\n",
      "['홍천']\n",
      "40\n",
      "['40']\n",
      "3도\n",
      "['3도']\n",
      "우리나라\n",
      "['우리', '리나', '나라']\n",
      "기상관측\n",
      "['기상', '상관', '관측']\n",
      "이래\n",
      "['이래']\n",
      "역대\n",
      "['역대']\n",
      "최고\n",
      "['최고']\n",
      "2보\n",
      "['2보']\n",
      "뉴스\n",
      "['뉴스']\n",
      "서울\n",
      "['서울']\n",
      "연합뉴스\n",
      "['연합', '합뉴', '뉴스']\n",
      "김승욱\n",
      "['김승', '승욱']\n",
      "기자\n",
      "['기자']\n",
      "강원도\n",
      "['강원', '원도']\n",
      "홍천의\n",
      "['홍천', '천의']\n",
      "수은주가\n",
      "['수은', '은주', '주가']\n",
      "1일\n",
      "['1일']\n",
      "40\n",
      "['40']\n",
      "3도까지\n",
      "['3도', '도까', '까지']\n",
      "치솟아\n",
      "['치솟', '솟아']\n",
      "기상관측\n",
      "['기상', '상관', '관측']\n",
      "이래\n",
      "['이래']\n",
      "역대\n",
      "['역대']\n",
      "최고기온을\n",
      "['최고', '고기', '기온', '온을']\n",
      "갈아치웠다\n",
      "['갈아', '아치', '치웠', '웠다']\n",
      "기상청에\n",
      "['기상', '상청', '청에']\n",
      "따르면\n",
      "['따르', '르면']\n",
      "이날\n",
      "['이날']\n",
      "오후\n",
      "['오후']\n",
      "1시\n",
      "['1시']\n",
      "59시께\n",
      "['59', '9시', '시께']\n",
      "40\n",
      "['40']\n",
      "1도를\n",
      "['1도', '도를']\n",
      "기록한\n",
      "['기록', '록한']\n",
      "뒤\n",
      "[]\n",
      "오후\n",
      "['오후']\n",
      "2시\n",
      "['2시']\n",
      "1분께\n",
      "['1분', '분께']\n",
      "40\n",
      "['40']\n",
      "3도로\n",
      "['3도', '도로']\n",
      "기온이\n",
      "['기온', '온이']\n",
      "더\n",
      "[]\n",
      "올랐다\n",
      "['올랐', '랐다']\n",
      "우리나라\n",
      "['우리', '리나', '나라']\n",
      "기상관측\n",
      "['기상', '상관', '관측']\n",
      "역대\n",
      "['역대']\n",
      "최고\n",
      "['최고']\n",
      "온도다\n",
      "['온도', '도다']\n",
      "부산\n",
      "['부산']\n",
      "인천\n",
      "['인천']\n",
      "1904년\n",
      "['19', '90', '04', '4년']\n",
      "서울\n",
      "['서울']\n",
      "1907년\n",
      "['19', '90', '07', '7년']\n",
      "등\n",
      "[]\n",
      "현대적인\n",
      "['현대', '대적', '적인']\n",
      "기상관측\n",
      "['기상', '상관', '관측']\n",
      "장비가\n",
      "['장비', '비가']\n",
      "도입된\n",
      "['도입', '입된']\n",
      "20세기\n",
      "['20', '0세', '세기']\n",
      "초반\n",
      "['초반']\n",
      "이래\n",
      "['이래']\n",
      "전국에서\n",
      "['전국', '국에', '에서']\n",
      "40도를\n",
      "['40', '0도', '도를']\n",
      "돌파한\n",
      "['돌파', '파한']\n",
      "적은\n",
      "['적은']\n",
      "1942년\n",
      "['19', '94', '42', '2년']\n",
      "8월\n",
      "['8월']\n",
      "1일\n",
      "['1일']\n",
      "대구\n",
      "['대구']\n",
      "40\n",
      "['40']\n",
      "0도\n",
      "['0도']\n",
      "가\n",
      "[]\n",
      "유일했다\n",
      "['유일', '일했', '했다']\n",
      "앞서\n",
      "['앞서']\n",
      "경북\n",
      "['경북']\n",
      "의성은\n",
      "['의성', '성은']\n",
      "올해\n",
      "['올해']\n",
      "7월\n",
      "['7월']\n",
      "27일\n",
      "['27', '7일']\n",
      "39\n",
      "['39']\n",
      "9도\n",
      "['9도']\n",
      "충북\n",
      "['충북']\n",
      "추풍령은\n",
      "['추풍', '풍령', '령은']\n",
      "1939년\n",
      "['19', '93', '39', '9년']\n",
      "7월\n",
      "['7월']\n",
      "21일\n",
      "['21', '1일']\n",
      "39\n",
      "['39']\n",
      "8도를\n",
      "['8도', '도를']\n",
      "기록한\n",
      "['기록', '록한']\n",
      "바\n",
      "[]\n",
      "있다\n",
      "['있다']\n",
      "08\n",
      "['08']\n"
     ]
    }
   ],
   "source": [
    "for file in fileList[:1]: \n",
    "    with open(file,'r',encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "        content = re.sub(r\"[\\s]{2,}\",\"\",content) # strip\n",
    "        for sentence in nltk.sent_tokenize(content):\n",
    "            words = nltk.regexp_tokenize(sentence, r\"[0-9ㄱ-ㅎㅏ-ㅣ가-힛]+\") # 한글 tokenizing을 위해 regexp를 쓴다\n",
    "            for word in words:\n",
    "                tokens = []\n",
    "                for i in range(len(word)):\n",
    "                    if i+1<len(word): # 1음절, 마지막음절 제거\n",
    "                        tokens.append(word[i:i+2]) # bigram으로 자름\n",
    "                print(word)\n",
    "                print(tokens, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Indexing\n",
    "![29](https://user-images.githubusercontent.com/38183218/43558021-5154a3e4-9642-11e8-8194-274bd2dc9a29.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    }
   ],
   "source": [
    "unq_nouns = [] # vetorize를 위해 unique한 noun을 뽑아낸다\n",
    "\n",
    "for sentence in nltk.sent_tokenize(content):\n",
    "    nouns = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        nouns.extend(Kkma().nouns(word))\n",
    "        nouns = list(set(nouns))\n",
    "    unq_nouns.extend(nouns)\n",
    "\n",
    "unq_nouns = list(set(unq_nouns))\n",
    "print(len(unq_nouns)) # 차원의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053\n"
     ]
    }
   ],
   "source": [
    "#docs 전체의 unq_nouns를 만들자\n",
    "for file in fileList:\n",
    "     with open(file,'r',encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "        content = re.sub(r\"[\\s]{2,}\",\"\",content) # strip\n",
    "        \n",
    "        for sentence in nltk.sent_tokenize(content):\n",
    "            nouns = []\n",
    "            for word in nltk.word_tokenize(sentence):\n",
    "                nouns.extend(Kkma().nouns(word))\n",
    "                nouns = list(set(nouns))\n",
    "            \n",
    "            unq_nouns.extend(nouns)\n",
    "            unq_nouns = list(set(unq_nouns))\n",
    "            \n",
    "print(len(unq_nouns)) # 차원의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_nouns = {}\n",
    "maxCount = {}\n",
    "\n",
    "for file in fileList:\n",
    "    docid = file[-17:]\n",
    "    maxCount[docid] = 0\n",
    "    with open(file,'r',encoding='utf-8') as fp:\n",
    "        content = fp.read()\n",
    "        content = re.sub(r\"[\\s]{2,}\",\"\",content) # strip\n",
    "        \n",
    "    nouns = {}\n",
    "    \n",
    "    for sentence in nltk.sent_tokenize(content): \n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            for w in Kkma().nouns(word):\n",
    "                if w in nouns.keys():\n",
    "                    nouns[w] += 1\n",
    "                else:\n",
    "                    nouns[w] = 1\n",
    "                    \n",
    "                if maxCount[docid] < nouns[w]:\n",
    "                    maxCount[docid] = nouns[w]\n",
    "                    \n",
    "    doc_nouns[docid] = nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\\news_docs0.txt - 98/6\n",
      "cs\\news_docs1.txt - 185/22\n",
      "s\\news_docs10.txt - 402/40\n",
      "s\\news_docs11.txt - 371/17\n",
      "cs\\news_docs2.txt - 135/9\n",
      "cs\\news_docs3.txt - 313/30\n",
      "cs\\news_docs4.txt - 123/7\n",
      "cs\\news_docs5.txt - 16/1\n",
      "cs\\news_docs6.txt - 163/7\n",
      "cs\\news_docs7.txt - 269/19\n",
      "cs\\news_docs8.txt - 463/24\n",
      "cs\\news_docs9.txt - 221/12\n"
     ]
    }
   ],
   "source": [
    "for k,v in doc_nouns.items():\n",
    "    print(\"{0} - {1}/{2}\".format(k, len(v), maxCount[k])) # 문서id-   등장단어수/가장 높았던 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cs\\\\news_docs0.txt', 'cs\\\\news_docs1.txt', 's\\\\news_docs10.txt', 's\\\\news_docs11.txt', 'cs\\\\news_docs2.txt', 'cs\\\\news_docs3.txt', 'cs\\\\news_docs4.txt', 'cs\\\\news_docs5.txt', 'cs\\\\news_docs6.txt', 'cs\\\\news_docs7.txt', 'cs\\\\news_docs8.txt', 'cs\\\\news_docs9.txt'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_nouns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 3,\n",
       " '1265': 1,\n",
       " '1265억원': 1,\n",
       " '1300': 1,\n",
       " '190': 1,\n",
       " '190억': 1,\n",
       " '1억1300만': 1,\n",
       " '21': 1,\n",
       " '21조': 1,\n",
       " '3': 1,\n",
       " '30': 2,\n",
       " '30년간': 1,\n",
       " '30일': 1,\n",
       " '3국': 1,\n",
       " '620': 3,\n",
       " '620억': 2,\n",
       " '620억달러': 1,\n",
       " '68': 1,\n",
       " '68조원': 1,\n",
       " '가운데': 1,\n",
       " '간': 1,\n",
       " '간섭': 1,\n",
       " '간여': 1,\n",
       " '개': 1,\n",
       " '개선': 1,\n",
       " '건설': 7,\n",
       " '격돌': 2,\n",
       " '견제': 2,\n",
       " '결국': 1,\n",
       " '경': 1,\n",
       " '경우': 1,\n",
       " '경제': 1,\n",
       " '고': 1,\n",
       " '과': 1,\n",
       " '관계': 1,\n",
       " '관심': 1,\n",
       " '구제': 7,\n",
       " '구제금융': 2,\n",
       " '국': 1,\n",
       " '국무': 1,\n",
       " '국무장관': 1,\n",
       " '국방장관': 1,\n",
       " '국제': 1,\n",
       " '국제통화기금': 1,\n",
       " '금융': 7,\n",
       " '기금': 1,\n",
       " '기술': 1,\n",
       " '기자': 1,\n",
       " '년': 1,\n",
       " '눈길': 1,\n",
       " '뉴스': 2,\n",
       " '뉴스1': 1,\n",
       " '니스': 1,\n",
       " '다대': 1,\n",
       " '다대다로': 1,\n",
       " '다로': 1,\n",
       " '달러': 5,\n",
       " '대': 1,\n",
       " '대부분': 1,\n",
       " '대테러': 1,\n",
       " '더': 2,\n",
       " '도': 1,\n",
       " '도로': 1,\n",
       " '돈': 2,\n",
       " '두': 1,\n",
       " '두고': 2,\n",
       " '등': 1,\n",
       " '등에': 1,\n",
       " '때': 1,\n",
       " '때문': 1,\n",
       " '로': 1,\n",
       " '를': 3,\n",
       " '만': 1,\n",
       " '매티스': 1,\n",
       " '무역': 1,\n",
       " '무역전쟁': 1,\n",
       " '미': 2,\n",
       " '미국': 22,\n",
       " '미국판': 2,\n",
       " '미중': 1,\n",
       " '박': 1,\n",
       " '박형기': 1,\n",
       " '반대': 3,\n",
       " '반응': 1,\n",
       " '발언': 1,\n",
       " '발전소': 2,\n",
       " '방해': 1,\n",
       " '법': 1,\n",
       " '변화': 1,\n",
       " '분쟁': 1,\n",
       " '비난': 1,\n",
       " '사업': 4,\n",
       " '사이': 1,\n",
       " '서로': 1,\n",
       " '서울': 1,\n",
       " '세력권': 1,\n",
       " '수': 2,\n",
       " '시사': 1,\n",
       " '신': 1,\n",
       " '신기술': 1,\n",
       " '신청': 1,\n",
       " '실제': 2,\n",
       " '아시아': 1,\n",
       " '안': 2,\n",
       " '암투': 1,\n",
       " '약': 1,\n",
       " '억': 6,\n",
       " '에너지': 1,\n",
       " '연결': 2,\n",
       " '연계': 2,\n",
       " '영향력': 1,\n",
       " '완공': 1,\n",
       " '완료': 1,\n",
       " '원': 3,\n",
       " '원래': 1,\n",
       " '위치': 2,\n",
       " '유지': 1,\n",
       " '의': 1,\n",
       " '의도': 1,\n",
       " '의심': 1,\n",
       " '이': 3,\n",
       " '이번': 2,\n",
       " '이상': 1,\n",
       " '이유': 2,\n",
       " '인도': 1,\n",
       " '인프라': 3,\n",
       " '일': 1,\n",
       " '일대': 4,\n",
       " '일대일': 3,\n",
       " '일대일로': 4,\n",
       " '일로': 4,\n",
       " '입가': 1,\n",
       " '장관': 2,\n",
       " '재무부': 1,\n",
       " '전략': 1,\n",
       " '전략적': 1,\n",
       " '전력': 1,\n",
       " '전쟁': 2,\n",
       " '점': 1,\n",
       " '점입가경': 1,\n",
       " '정부': 2,\n",
       " '정책': 1,\n",
       " '제': 1,\n",
       " '제공': 1,\n",
       " '제임스': 1,\n",
       " '조': 2,\n",
       " '조건': 1,\n",
       " '주도': 1,\n",
       " '주요': 1,\n",
       " '주장': 2,\n",
       " '중': 2,\n",
       " '중국': 19,\n",
       " '중동': 2,\n",
       " '중요': 1,\n",
       " '증대': 1,\n",
       " '지난달': 1,\n",
       " '지난주': 1,\n",
       " '지역': 2,\n",
       " '지정학': 1,\n",
       " '진행': 1,\n",
       " '차관': 2,\n",
       " '차지': 2,\n",
       " '채권자': 1,\n",
       " '총': 1,\n",
       " '최근': 3,\n",
       " '추진': 2,\n",
       " '측': 1,\n",
       " '치열': 1,\n",
       " '태평양': 1,\n",
       " '테러': 1,\n",
       " '통화': 1,\n",
       " '투자': 3,\n",
       " '파키스탄': 20,\n",
       " '판': 2,\n",
       " '폼페이': 1,\n",
       " '프가': 1,\n",
       " '프가니스': 1,\n",
       " '프로그램': 1,\n",
       " '한': 1,\n",
       " '함': 1,\n",
       " '행보': 1,\n",
       " '형기': 1,\n",
       " '형태': 1,\n",
       " '회랑': 1,\n",
       " '후': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_nouns['cs\\\\news_docs1.txt'] # 문서에 등장 단어와 빈도를 딕셔너리로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inverted Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![28](https://user-images.githubusercontent.com/38183218/43557954-1fcd53de-9642-11e8-9c8e-bb5f9aebc331.jpg)\n",
    "![30](https://user-images.githubusercontent.com/38183218/43557955-1ffc6a0c-9642-11e8-97ed-6e2f78c07d8b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "invertedIdx = {}\n",
    "for noun in unq_nouns:\n",
    "    invertedIdx[noun] = []\n",
    "    \n",
    "    for k,v in doc_nouns.items():\n",
    "        if noun in v:\n",
    "            invertedIdx[noun].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs\\\\news_docs1.txt'] ['cs\\\\news_docs0.txt', 'cs\\\\news_docs2.txt', 'cs\\\\news_docs5.txt']\n"
     ]
    }
   ],
   "source": [
    "print(invertedIdx['구제금융'], invertedIdx['강원도']) #단어기준으로 문서찾는 inverted 인덱싱이 완료됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf와 idf 모두 보정이 필요, 문서 수 가 적은 경우에는 tf만 보정하기도 한다\n",
    "![34](https://user-images.githubusercontent.com/38183218/43563535-8e714410-965d-11e8-998d-ee8c8da5957a.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 0.5 # K + (1-K)*(f(t,d)/maxf(t,d)) 최솟값 0.5, 최댓값 1 \n",
    "TF = {}\n",
    "for k,v in doc_nouns.items():\n",
    "    tfList = {}\n",
    "    \n",
    "    for w in v:\n",
    "        tfList[w] =  K + (1-K)*(v[w]/maxCount[k])\n",
    "#         print('{0} | {1} + {2} *({3}/{4})  =  {5}'.format(\n",
    "#             w, K, (1-K), v[w], maxCount[k], tfList[w]\n",
    "#         ))\n",
    "        \n",
    "    TF[k] = tfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docSize = len(fileList)\n",
    "TFIDF = {}\n",
    "\"\"\"\n",
    "TF = doc1|{단어1:0.54, 단어2:056 ...} \n",
    "\"\"\"\n",
    "for (k,v) in TF.items():\n",
    "    idfList = {}\n",
    "    for t in v:\n",
    "        idf = log10(docSize/len(invertedIdx[t]))\n",
    "        idfList[t] = v[t] * idf\n",
    "#         print('{0} | {1} * log({2} / {3}) = {4}'.format(\n",
    "#             t, v[t], docSize, len(invertedIdx[t]), idfList[t]\n",
    "#         ))\n",
    "    \n",
    "    TFIDF[k] = idfList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedByWeight = [sorted(dic.items(), key=lambda kv:kv[1],reverse=True) for dic in TFIDF.values()] # 내림차순 정렬해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('미국', 1.0791812460476249),\n",
       " ('파키스탄', 1.03012755304546),\n",
       " ('구제', 0.711278548531389),\n",
       " ('건설', 0.711278548531389),\n",
       " ('일대일로', 0.637698009028142),\n",
       " ('일로', 0.637698009028142),\n",
       " ('인프라', 0.6131711625270595),\n",
       " ('620', 0.6131711625270595),\n",
       " ('일대일', 0.6131711625270595),\n",
       " ('미', 0.5886443160259772)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedByWeight[1][:10] #1번 doc의 TFIDF기반 중요단어 top10을 뽑았다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity: 벡터의 방향을 기준으로 similarity도출, 1사분면만 쓰기에(음수값없음) 0~1 사이의 값을 가짐\n",
    "![39](https://user-images.githubusercontent.com/38183218/43562271-946b5c7c-9656-11e8-8f6c-221b5602d2dc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B vector는 쿼리를 의미, 모든 경우에 다 필요하므로 연산안해도 된다\n",
    "\n",
    "쿼리를 받아서 똑같이 전처리 한 뒤, tf-idf매긴다... AdHoc모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'미국': 2, '중국': 1, '중': 1, '라': 1}\n"
     ]
    }
   ],
   "source": [
    "query = '미국과 중국 중 미국 이겨라'\n",
    "\n",
    "query_nouns = {}\n",
    "maxCount = 0\n",
    "    \n",
    "for word in nltk.word_tokenize(query):\n",
    "    for w in Kkma().nouns(word):\n",
    "        if w in query_nouns.keys():\n",
    "            query_nouns[w] += 1\n",
    "        else:\n",
    "            query_nouns[w] = 1\n",
    "                    \n",
    "        if maxCount < query_nouns[w]:\n",
    "            maxCount = query_nouns[w]\n",
    "    \n",
    "print(query_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쿼리를 한 doc으로 보고 가중치를 구한다, 여기서는 tf만 구했다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국 | 0.5 + 0.5 *(2/2)  =  1.0\n",
      "중국 | 0.5 + 0.5 *(1/2)  =  0.75\n",
      "중 | 0.5 + 0.5 *(1/2)  =  0.75\n",
      "라 | 0.5 + 0.5 *(1/2)  =  0.75\n"
     ]
    }
   ],
   "source": [
    "query_weight = {}\n",
    "\n",
    "for k,v in query_nouns.items():\n",
    "    query_weight[k] = K + (1-K) * (v/maxCount)\n",
    "    print('{0} | {1} + {2} *({3}/{4})  =  {5}'.format(\n",
    "            k, K, (1-K), v, maxCount, query_weight[k]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc의 len(문서별 단어 가중치의 유클리디언 합) 구해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_len = {} #문서1:길이, 문서2:길이 ...\n",
    "\n",
    "for k,v in TFIDF.items(): #k는 문서id, v는 단어를 키, 가중치를 밸류로 가진 딕셔너리\n",
    "    sumPow = 0\n",
    "    \n",
    "    for t in v:\n",
    "        sumPow += v[t]**2\n",
    "    \n",
    "    doc_len[k] = sqrt(sumPow)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cs\\\\news_docs0.txt': 4.899413355380067, 'cs\\\\news_docs1.txt': 6.558866887131949, 's\\\\news_docs10.txt': 10.312014879907268, 's\\\\news_docs11.txt': 10.15287536836188, 'cs\\\\news_docs2.txt': 5.788706649529996, 'cs\\\\news_docs3.txt': 8.366022889355873, 'cs\\\\news_docs4.txt': 5.960473392924283, 'cs\\\\news_docs5.txt': 2.1381360132336273, 'cs\\\\news_docs6.txt': 7.023185415453959, 'cs\\\\news_docs7.txt': 8.33759189891698, 'cs\\\\news_docs8.txt': 11.194428670049708, 'cs\\\\news_docs9.txt': 7.822790935307187}\n"
     ]
    }
   ],
   "source": [
    "print(doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미국\n",
      "docid:cs\\news_docs1.txt, weight:1.0791812460476249\n",
      "중국\n",
      "docid:cs\\news_docs1.txt, weight:0.4445902600796855\n",
      "docid:s\\news_docs10.txt, weight:0.298200784199789\n",
      "docid:cs\\news_docs3.txt, weight:0.24651264827182562\n",
      "docid:cs\\news_docs8.txt, weight:0.2783207319198031\n",
      "중\n",
      "docid:cs\\news_docs1.txt, weight:0.26024795711981585\n",
      "docid:s\\news_docs10.txt, weight:0.2504886587278228\n",
      "docid:cs\\news_docs6.txt, weight:0.47712125471966244\n",
      "docid:cs\\news_docs9.txt, weight:0.25844067963981715\n",
      "라\n",
      "docid:s\\news_docs10.txt, weight:0.40852940645141295\n",
      "docid:cs\\news_docs6.txt, weight:0.4446578573620821\n",
      "\n",
      " {'cs\\\\news_docs1.txt': 1.6078099089472508, 's\\\\news_docs10.txt': 0.7179141370342685, 'cs\\\\news_docs3.txt': 0.1848844862038692, 'cs\\\\news_docs8.txt': 0.20874054893985233, 'cs\\\\news_docs6.txt': 0.6913343340613084, 'cs\\\\news_docs9.txt': 0.19383050972986288}\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for k,v in query_weight.items():\n",
    "    if k in invertedIdx.keys():\n",
    "        print(k)\n",
    "\n",
    "        for docid in invertedIdx[k]:\n",
    "            if docid in result.keys():\n",
    "                result[docid] += TFIDF[docid][k] * query_weight[k]\n",
    "            else:\n",
    "                result[docid] = TFIDF[docid][k] * query_weight[k]\n",
    "\n",
    "            print('docid:{0}, weight:{1}'.format(docid,TFIDF[docid][k]))\n",
    "            \n",
    "print('\\n',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cs\\news_docs1.txt] 유사도:0.005698338577228374\n",
      "[cs\\news_docs3.txt] 유사도:0.0003157499753311188\n",
      "[cs\\news_docs6.txt] 유사도:0.0019956561901775358\n",
      "[cs\\news_docs8.txt] 유사도:0.0001487993455287504\n",
      "[cs\\news_docs9.txt] 유사도:0.00040488992135811976\n",
      "[s\\news_docs10.txt] 유사도:0.0006546993540152517\n"
     ]
    }
   ],
   "source": [
    "result.sort()\n",
    "\n",
    "for k,v in result:\n",
    "    print(\"[{0}] 유사도:{1}\".format(k,v)) # 해당문서와 쿼리간의 정렬된 코사인 유사도를 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미중 분쟁 점입가경, 이번에는 파키스탄 두고 격돌 | Daum 뉴스\n",
      "(서울=뉴스1) 박형기 기자 = 최근 무역전쟁을 벌이고 있는 미국과 중국이 이번에는 파키스탄을 두고 격돌하고 있다.파키스탄이 미국이 주도하고 있는 국제통화기금(IMF)에 구제금융 신청하자 미국이 이를 반대하고 나선 것. 파키스탄이 IMF의 구제 금융을 받으면 그 돈이 모두 중국으로 흘러들어갈 것이란 이유에서다.◇ 미국, 파키스탄 IMF 구제금융 반대 : 실제 중국은 일대일로를 건설하면서 파키스탄에 막대한 투자를 하고 있다. 파키스탄은 바로 중동과 연결되기 때문에 일대일로에서 전략적인 위치를 차지하고 있다. 중국은 그런 파키스탄의 인프라 건설을 위해 620억 달러(68조원)를 차관 등의 형태로 제공하고 있다.미국은 파키스탄이 IMF의 구제 금융을 받으면 그 돈이 결국은 채권자인 중국에 흘러들어갈 것이란 이유로 IMF 구제 금융을 반대하고 있는 것이다.그러나 파키스탄은 미국이 중국과 파키스탄의 사이를 벌리려 하고 있다고 비난하고 나섰다.파키스탄은 원래 미국이 아프가니스탄과 대테러 전쟁을 펼칠 때, 미국과 좋은 관계를 유지했으나 최근 중국이 파키스탄에 막대한 투자를 함에 따라 중국과의 관계가 급속히 개선되고 있다.◇ 중국-파키스탄 620억달러 CPEC 사업 추진 : 중국은 현재 파키스탄과 총 620억 달러에 달하는 ‘중국 파키스탄 경제 회랑(CPEC)’ 건설 사업을 추진하고 있다. 이 가운데 190억 달러(약 21조 원)의 도로 및 발전소 건설 사업을 진행 중이거나 완료했다.이들 인프라 건설 프로그램은 대부분 중국 측의 차관으로 건설된다. 그러나 발전소의 경우, 완공 후 30년간 파키스탄이 전력을 사들이는 조건으로 건설되고 있다. 파키스탄 정부는 IMF 구제 금융과 CPEC를 연계하려는 미국의 의도에 불쾌한 반응을 보이고 있다. 따라서 파키스탄 재무부는 IMF 구제 금융과 CPEC을 연계하는 것은 터무니없는 주장이며, 미국 같은 제 3국이 이에 간여해서는 안 될 것이라고 주장하고 있다.중국도 미국의 이 같은 행보에 의심의 눈길을 보내고 있다. 중국은 미국이 CPEC 사업에 간섭해 중국의 일대일로 정책을 방해하려 하고 있다고 보고 있다.◇ 미국 ‘미국판 일대일로’로 중국 견제 : 실제 미국은 최근 중국의 일대일로에 맞서기 위해 ‘미국판 일대일로’를 들고 나왔다. 폼페이오 국무장관은 지난달 30일 미국이 인도-태평양 지역에 더 많은 관심을 가질 것이며, 신기술, 에너지, 인프라 등에 1억1300만 달러(1265억원)를 투자할 것이라고 밝혔다.앞서 제임스 매티스 미 국방장관도 지난주 “일대일로만 있으란 법은 없다. 두 개 이상의 일대일로가 있을 수 있으며, 더 나아가 ‘다대다로’도 있을 수 있다”고 말했다.미국의 주요 장관들이 잇따라 이 같은 발언을 한 것은 미국 정부의 전략 변화를 시사하는 것이다. 미국은 아시아 지역에서 미국의 영향력을 증대시킴으로써 중국을 견제하려는 것이다.미국과 중국이 중동과 바로 연결되는 중요한 지정학적 위치를 차지하고 있는 파키스탄을 서로의 세력권 안에 두기 위해 치열한 암투를 벌이고 있는 것이다.sinopark@news1.kr\n"
     ]
    }
   ],
   "source": [
    "with open(fileList[1],encoding='utf-8') as fp:\n",
    "    print(fp.read()) #쿼리와 가장 유사도가 높은 뉴스이다, 대충맞다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "언어처리 모델도 모델, 성능평가 해야한다\n",
    "\n",
    "언어처리하는 모델에서는 accuracy 전혀 무의미, precision과 recall을 봐야함\n",
    "![44](https://user-images.githubusercontent.com/38183218/43564599-68e8031e-9662-11e8-8c4b-10d1946b213e.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
