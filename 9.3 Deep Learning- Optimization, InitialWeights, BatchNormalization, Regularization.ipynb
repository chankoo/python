{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizaition Methods\n",
    "![7](https://user-images.githubusercontent.com/38183218/44017435-440dd208-9f13-11e8-8d7c-0fc626274bc8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://user-images.githubusercontent.com/38183218/44017423-41111d30-9f13-11e8-9181-b582c4973096.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent(full batch와 비교)\n",
    "- 배치마다 gradient 내려가는 모양다르기에 local minima에 빠지는 위험 피할수있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- 그러나\n",
    "\n",
    "![1 5](https://user-images.githubusercontent.com/38183218/44017424-413a8de6-9f13-11e8-9117-65d2c48b8d4c.PNG)\n",
    "![2](https://user-images.githubusercontent.com/38183218/44017425-41632972-9f13-11e8-861d-fc9ad89036b7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD의 단점은 비등방성함수(방향에 따라 성질, 즉 기울기가 달라지는 함수)에서 탐색 경로가 비효율적\n",
    "![](https://user-images.githubusercontent.com/38183218/45130977-e3017f80-b1c5-11e8-8269-27574584cf26.png)\n",
    "- 예들어 위와같이 y축방향 기울기 가파르지만 x축방향 기울기는 완만한경우. 최적점인 (0,0)에 도달하는 시간이 매우 오래걸림\n",
    "\n",
    "- 근본적인 한계도 존재, 그레디언트의 방향이 최적점과 다른방향을 가리키는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum Update\n",
    "같은 방향이면 쭉 밀어주기\n",
    "![3](https://user-images.githubusercontent.com/38183218/44017426-418b4b64-9f13-11e8-92cb-3ec0ca5b61b7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "            self.lr = lr\n",
    "            self.momentum = momentum\n",
    "            self.v = None\n",
    "            \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zero_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad Update\n",
    "노드마다 lr다르게 걸어서 학습률 최적화. 보통 학습률을 서서히 낮추는 형태. \n",
    "\n",
    "직관적으로 생각해도 최적점찾으려면 처음에는 보폭을 크게, 나중에는 작게해야.\n",
    "\n",
    "큰 움직임에 비례해 갱신 정도도 큰폭으로 작아진다.\n",
    "\n",
    "그래서 SGD와 같이 지그재그 형태로 움직이더라도 y축방향으로 갱신강도가 빠르게 약해지고, 지그재그의 움직임이 줄어든다\n",
    "\n",
    "cache값이 점점 증가하므로 어느순간 dx가 0 되는 단점 -> RMSProp으로 해결\n",
    "\n",
    "![4](https://user-images.githubusercontent.com/38183218/44017432-439170aa-9f13-11e8-869d-bddbaea10c9b.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp Update\n",
    "기울기를 제곱하여 계속 더해가는 AdaGrad의 cache, 그래서 학습 진행할수록 갱신강도가 약해진다\n",
    "\n",
    "실제로 어느순간 갱신량이 0이된다\n",
    "\n",
    "RMSProp은 지수이동평균(EMA)을 사용하여 과거 기울기의 반영 정도를 기하급수적으로 감소시킨다\n",
    "\n",
    "decay rate를 사용\n",
    "![5](https://user-images.githubusercontent.com/38183218/44017433-43bab082-9f13-11e8-9fb1-ad410728df20.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Update\n",
    "\n",
    "Adam Optimization 논문 요약/정리 : http://dalpo0814.tistory.com/29\n",
    "\n",
    "모멘텀 + RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam 간단히 구현\n",
    "m = beta1*m + (1-beta1)*dx \n",
    "v = beta2*v + (1-beta2)*(dx**2) \n",
    "x -= learning_Rate * m/(np.sqrt(v) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/45130955-e0068f00-b1c5-11e8-8a4e-cfb1d55eb9cb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Weights\n",
    "\n",
    "__weight decay__ 라는것이 필요 : 가중치 값을 작게하여 오버피팅을 억제하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그렇다면 Initial Weights를 0으로하면 어떨까?\n",
    "- 학습이 안됨\n",
    "- 정확히말하면 가중치를 균일한 값으로 설정하면 안됨, 모든 가중치가 똑같이 갱신됨\n",
    "- 대칭적인 구조로 이루어지는 역전파(곱셈, 덧셈노드)이기에 대칭구조를 깰려면 가중치의 초깃값을 무작위로 설정해야한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### activation값의 분포\n",
    "- 표준편차 1인 정규분포로 초기화한 가중치의 activation값\n",
    "![](https://user-images.githubusercontent.com/38183218/45130958-e0068f00-b1c5-11e8-8f4b-4f70fcc5940d.png)\n",
    "\n",
    "- 시그모이드 함수의 경우 출력이 0 또는 1에 가까워질수록 미분이 0에 다가감.\n",
    "- -> __gradient vanishing__ 의 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 반면 표준편차 0.01인 정규분포로 초기화한 가중치의 activation값\n",
    "- 0.5부근에 집중되어 gradient vanishing은 일어나지 않음\n",
    "- 그러나 대부분의 뉴런이 같은값을 출력하므로 __표현력이 제한__ 되는 문제\n",
    "![](https://user-images.githubusercontent.com/38183218/45130959-e09f2580-b1c5-11e8-9c4f-ed5e59bb2403.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Xavier 초깃값__\n",
    "- 노드개수가 n일때 표준편차가 1/sqrt(n)인 정규분포로 가중치를 초기화\n",
    "![](https://user-images.githubusercontent.com/38183218/45130961-e09f2580-b1c5-11e8-9ccb-eb641421e6da.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLu 사용시의 가중치 초깃값 \n",
    "- He 초깃값 -> 표준편차로 sqrt(2/n)\n",
    "- Xavier 초깃값은 활성화함수가 선형이거나 선형과 유사한 성질(sigmoid, tanh는 중앙부근이 선형)일때만 사용\n",
    "![](https://user-images.githubusercontent.com/38183218/45130962-e09f2580-b1c5-11e8-92ae-08607dc21141.png)\n",
    "\n",
    "- He 초깃값인 경우 모든 층에서 활성화값이 균일하게 분포\n",
    "- 결론적으로 ReLu일땐 He, sigmoid나 tanh일땐 Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "원활한 학습위해선 활성화값의 분포가 적당히 퍼져있어야함\n",
    "\n",
    "그렇다면 각 층이 활성화값을 적당히 퍼뜨리도록 '강제'하면 어떠냐\n",
    "\n",
    "-> Batch Normalization의 아이디어\n",
    "\n",
    "즉, 레이어가 쌓일 수록 이상치인 batch가 가져오는 오차는 커지니까\n",
    "\n",
    "그래서 애초에 각층마다 배치정규화 계층넣어서 zero-centered로 normalize해버리자\n",
    "\n",
    "- 학습속도개선\n",
    "- 초기값에 크게 의존x\n",
    "- 오버피팅 억제해 드롭아웃 등의 필요성 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/45130964-e137bc00-b1c5-11e8-8b55-09e6941e022d.png)\n",
    "배치정규화 계층을 신경망에 삽입\n",
    "\n",
    "미니배치단위로 정규화(평균 0, 분산1의 표준화까지)\n",
    "\n",
    "결과로 빠른 학습속도\n",
    "![](https://user-images.githubusercontent.com/38183218/45130967-e1d05280-b1c5-11e8-880d-cc823036469c.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "오버피팅 방지 위함\n",
    "\n",
    "__오버피팅 발생 가능성__\n",
    "- 매개변수가 많고 표현력이 높은 모델\n",
    "- 훈련데이터가 적은 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅 발생시\n",
    "![](https://user-images.githubusercontent.com/38183218/45130971-e268e900-b1c5-11e8-8d1a-fb379302f5bd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가중치 감소(weight decay)\n",
    "- 전통적인 오버피팅 억제방법\n",
    "- 학습 과정에서 큰 가중치에 대해 큰 페널티 부과\n",
    "- 가중치 매개변수의 값이 커서 발생하는 오버피팅이 많은데 이를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/45263987-9e007600-b46f-11e8-9dec-0a305e9a529d.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 각각의 손실 함수에 1/2 \\* lambda \\* $W^2$ 을 더한다\n",
    "\n",
    "따라서 역전파 시에 정규화 항을 미분한 lambda\\*W 항이 더해진다\n",
    "\n",
    "아래 그래프는 lambda =0.1로 가중치 감소 적용한 결과\n",
    "\n",
    "-> 오버피팅 정도가 완화\n",
    "![](https://user-images.githubusercontent.com/38183218/45130972-e268e900-b1c5-11e8-9978-d288dfdb4514.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 드롭아웃\n",
    "- 복잡한 신경망에는 가중치 감소만으로 대응하기 어려움\n",
    "- 그럴땐 뉴런을 임의로 삭제하며 학습하는 방법인 드롭아웃을 이용한다\n",
    "- 학습 중에 hidden layer 의 뉴런을 무작위로 삭제하여 신호 전달을 못하게 한다\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/38183218/45130974-e268e900-b1c5-11e8-8c40-2025c72e1ceb.png)\n",
    "\n",
    "오른쪽이 dropout_ratio = 0.15로 드롭아웃을 적용한 결과이다. 오버피팅이 억제되었다\n",
    "![](https://user-images.githubusercontent.com/38183218/45130975-e3017f80-b1c5-11e8-97f0-a11845c696f6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembles\n",
    "- 일반적인 기계학습에서의 앙상블과 같이 개별적으로 학습시킨 여러 신경망의 출력을 평균(혹은 voting)하여 추론한다\n",
    "\n",
    "- 예를 들면 같은(혹은 비슷한) 구조의 신경망 m개를 준비하여 개별적으로 학습시키고, 테스트 시에 m개 출력의 평균을 내어 답하는 것\n",
    "\n",
    "- 드롭아웃과 밀접한 관련이 있는데, 드롭아웃이 뉴런을 무작위 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문\n",
    "![](https://user-images.githubusercontent.com/38183218/45263988-9e990c80-b46f-11e8-9d69-ee0e8987ea41.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "![](https://user-images.githubusercontent.com/38183218/45263989-9e990c80-b46f-11e8-80cc-e16068f268df.PNG)\n",
    "\n",
    "Ramdom mix of \n",
    "- translation\n",
    "- rotation\n",
    "- stretching\n",
    "- shearing\n",
    "- lens distortion..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
