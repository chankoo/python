{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizaition Methods\n",
    "![7](https://user-images.githubusercontent.com/38183218/44017435-440dd208-9f13-11e8-8d7c-0fc626274bc8.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![1](https://user-images.githubusercontent.com/38183218/44017423-41111d30-9f13-11e8-9181-b582c4973096.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent(full batch와 비교)\n",
    "- 배치마다 gradient 내려가는 모양다르기에 local minima에 빠지는 위험 피할수있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- 그러나\n",
    "\n",
    "![1 5](https://user-images.githubusercontent.com/38183218/44017424-413a8de6-9f13-11e8-9117-65d2c48b8d4c.PNG)\n",
    "![2](https://user-images.githubusercontent.com/38183218/44017425-41632972-9f13-11e8-861d-fc9ad89036b7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD의 단점은 비등방성함수(방향에 따라 성질, 즉 기울기가 달라지는 함수)에서 탐색 경로가 비효율적\n",
    "![](https://user-images.githubusercontent.com/38183218/45130977-e3017f80-b1c5-11e8-8269-27574584cf26.png)\n",
    "- 예들어 위와같이 y축방향 기울기 가파르지만 x축방향 기울기는 완만한경우. 최적점인 (0,0)에 도달하는 시간이 매우 오래걸림\n",
    "\n",
    "- 근본적인 한계도 존재, 그레디언트의 방향이 최적점과 다른방향을 가리키는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Momentum Update\n",
    "같은 방향이면 쭉 밀어주기\n",
    "![3](https://user-images.githubusercontent.com/38183218/44017426-418b4b64-9f13-11e8-92cb-3ec0ca5b61b7.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "            self.lr = lr\n",
    "            self.momentum = momentum\n",
    "            self.v = None\n",
    "            \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zero_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
    "            params[key] += self.v[key] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adagrad Update\n",
    "노드마다 lr다르게 걸어서 학습률 최적화. 보통 학습률을 서서히 낮추는 형태. \n",
    "\n",
    "직관적으로 생각해도 최적점찾으려면 처음에는 보폭을 크게, 나중에는 작게해야.\n",
    "\n",
    "큰 움직임에 비례해 갱신 정도도 큰폭으로 작아진다.\n",
    "\n",
    "그래서 SGD와 같이 지그재그 형태로 움직이더라도 y축방향으로 갱신강도가 빠르게 약해지고, 지그재그의 움직임이 줄어든다\n",
    "\n",
    "cache값이 점점 증가하므로 어느순간 dx가 0 되는 단점 -> RMSProp으로 해결\n",
    "\n",
    "![4](https://user-images.githubusercontent.com/38183218/44017432-439170aa-9f13-11e8-869d-bddbaea10c9b.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp Update\n",
    "기울기를 제곱하여 계속 더해가는 AdaGrad의 cache, 그래서 학습 진행할수록 갱신강도가 약해진다\n",
    "\n",
    "실제로 어느순간 갱신량이 0이된다\n",
    "\n",
    "RMSProp은 지수이동평균(EMA)을 사용하여 과거 기울기의 반영 정도를 기하급수적으로 감소시킨다\n",
    "\n",
    "decay rate를 사용\n",
    "![5](https://user-images.githubusercontent.com/38183218/44017433-43bab082-9f13-11e8-9fb1-ad410728df20.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam Update\n",
    "\n",
    "Adam Optimization 논문 요약/정리 : http://dalpo0814.tistory.com/29\n",
    "\n",
    "모멘텀 + RMSprop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam 간단히 구현\n",
    "m = beta1*m + (1-beta1)*dx \n",
    "v = beta2*v + (1-beta2)*(dx**2) \n",
    "x -= learning_Rate * m/(np.sqrt(v) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/45130955-e0068f00-b1c5-11e8-8a4e-cfb1d55eb9cb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "레이어가 쌓일 수록 이상치인 batch가 가져오는 오차는 커진다\n",
    "\n",
    "그래서 애초에 zero-centered로 normalize해버리면 어떠냐\n",
    "\n",
    ": 각 층이 활성화를 적당히 퍼뜨리도록 '강제' 하는 것\n",
    "- 학습속도개선\n",
    "- 초기값에 크게 의존x\n",
    "- 오버피팅 억제해 드롭아웃 등의 필요성 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "#### Ensembles\n",
    "#### Add term to loss\n",
    "#### Dropout\n",
    "어떤 feature의 가중치가 너무 커지면 drop하여 오버피팅 문제해결\n",
    "#### Data Augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
