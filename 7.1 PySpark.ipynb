{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "- Hadoop vs Spark\n",
    "- RDD\n",
    "- DataFrame\n",
    "- Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![33](https://user-images.githubusercontent.com/38183218/43623870-9e63e870-971e-11e8-9e72-9a381ead2831.jpg)\n",
    "하둡: HDFS 파일, 분산환경저장 기술, 맵리듀스\n",
    "\n",
    "스파크는 원래 하둡 위의 작은 서브프로젝트엿음 – 맵리듀스 외에도 여러 기능들 \n",
    "\n",
    "![34](https://user-images.githubusercontent.com/38183218/43623871-9e931d48-971e-11e8-92e8-2ece8cf16283.jpg)\n",
    "하둡은 서드파티(인터페이스)들을 다알아야 실질적으로 써먹는다. \n",
    "\n",
    "진입장벽 매우높음\n",
    "\n",
    "![35](https://user-images.githubusercontent.com/38183218/43623872-9ec538dc-971e-11e8-904a-fa39a15577a5.png)\n",
    "스파크만 알면 다른 서드파티 몰라도 처리가능. 강력함\n",
    "\n",
    "![36](https://user-images.githubusercontent.com/38183218/43623873-9f065d62-971e-11e8-8f56-203c096f176c.png)\n",
    "하둡 : 저장인프라\n",
    "\n",
    "스파크: 분산처리 도구\n",
    "\n",
    "![39](https://user-images.githubusercontent.com/38183218/43623874-9f2f78d2-971e-11e8-9c95-26cdb1f24b37.png)\n",
    "기존데이터에서 키밸류쌍을 가져온 맵,\n",
    "\n",
    "특정 키에 대해서 밸류값을 합쳐오는 리듀스\n",
    "\n",
    "![46](https://user-images.githubusercontent.com/38183218/43624798-95d4261a-9724-11e8-96e7-3d56e9dea49d.png)\n",
    "\n",
    "![47](https://user-images.githubusercontent.com/38183218/43624799-95fbdcaa-9724-11e8-88fe-11a3ad7d6a83.png) \n",
    "인메모리방식, 속도빠름\n",
    "\n",
    "자바만 지원하는 하둡과 달리 여러 언어지원, 써드파티 불필요\n",
    "\n",
    "여러 os지원, 리눅스만 지원하는 하둡과 차이\n",
    "\n",
    "run everywhere: 어디든 갖다붙음\n",
    "\n",
    "\n",
    "![50](https://user-images.githubusercontent.com/38183218/43624800-96224ac0-9724-11e8-97c2-ff0572819215.jpg)\n",
    "Lazy-execution\n",
    "\n",
    "#### Spark Core\n",
    "![51](https://user-images.githubusercontent.com/38183218/43624801-9647b440-9724-11e8-9b58-06d336a18634.png)\n",
    "\n",
    "- Driver\n",
    "    - SparkContext 생성/관리\n",
    "    - SparkContext – Spark 시작점, 메인 프로그램, Cluster 연결, 병렬 작업 분산(DAG)\n",
    "- Worker\n",
    "    - 하나의 물리적/논리적 노드, Executors 실행\n",
    "    - Executor – Tasks를 수행/저장하기 위한 하나의 Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![53](https://user-images.githubusercontent.com/38183218/43624802-966dcaae-9724-11e8-9500-1cd25f5c6964.PNG)\n",
    "\n",
    "![54](https://user-images.githubusercontent.com/38183218/43624803-96952ca2-9724-11e8-94bf-78fbf69dc7d8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark execution\n",
    "(base) C:\\Users\\Chankoo>cd ..\n",
    "\n",
    "(base) C:\\Users>cd ..\n",
    "\n",
    "(base) C:\\>cd opt/\n",
    "\n",
    "(base) C:\\opt>cd spark\n",
    "\n",
    "(base) C:\\opt\\spark>cd spark-2.3.1-bin-hadoop2.7\n",
    "\n",
    "(base) C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7>cd bin\n",
    "\n",
    "(base) C:\\opt\\spark\\spark-2.3.1-bin-hadoop2.7\\bin>pyspark --master local[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### driver setting\n",
    "![](https://user-images.githubusercontent.com/38183218/43624783-78bfb7ba-9724-11e8-9928-227fb79c11f7.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43624784-790e8714-9724-11e8-8b53-396534db1f63.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD\n",
    "![58](https://user-images.githubusercontent.com/38183218/43624805-96e8ba98-9724-11e8-9016-1cdb2c8b6b87.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/43626415-afb2926c-972c-11e8-80bd-a848a8860617.PNG)\n",
    "\n",
    "![78](https://user-images.githubusercontent.com/38183218/43628879-777da058-9736-11e8-8533-b580d7b3f930.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "- map\n",
    "    - RDD의 각 엘리먼트에 적용 리스트로 리턴 (결과를 튜플, 딕셔너리, 리스트 등으로 감싸야 한다)\n",
    "    \n",
    "- filter\n",
    "    - 특정 조건에 맞는 엘리먼트 선택\n",
    "\n",
    "![64](https://user-images.githubusercontent.com/38183218/43626422-b32f7ac2-972c-11e8-90c8-62f4a177ac66.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatMap\n",
    "    - map과 유사하지만, 리스트가 아닌 평면화 된 결과를 리턴\n",
    "    - 입력 데이터를 파싱 할 때 올바르지 않은 형태의 데이터를 제거하기 위해 사용\n",
    "\n",
    "![65](https://user-images.githubusercontent.com/38183218/43626423-b358bcb6-972c-11e8-8137-47a814a6ceeb.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- distinct\n",
    "    - 특정 열에서 중복된 값을 제거하여 고유한 값을 리스트로 리턴\n",
    "    - 많은 리소스 사용, 데이터를 섞는 연산 포함 정말 필요할 때 제한적으로 사용\n",
    "    \n",
    "- sample\n",
    "    - 데이터셋으로부터 임의로 추출된 샘플을 리턴\n",
    "    - (중복 허용 여부, 크기 비율, 랜덤시드)\n",
    "\n",
    "\n",
    "![66](https://user-images.githubusercontent.com/38183218/43626426-b37e17ea-972c-11e8-8472-b01eed678ed3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- leftOuterJoin\n",
    "    - 두 RDD가 매칭되는 데이터에 대해 왼쪽 RDD에 오른쪽 RDD 추가된 결과 리턴\n",
    "    - distinct()처럼 데이터를 섞는 연산 포함 정말 필요할 때 제한적으로 사용\n",
    "\n",
    "![67](https://user-images.githubusercontent.com/38183218/43626427-b3ca6280-972c-11e8-80c3-53cf8ebc2d7b.png)\n",
    "\n",
    "- join\n",
    "    - 두 RDD의 교집합 키에 대한 값\n",
    "- intersection\n",
    "    - 두 RDD의 교집합\n",
    "\n",
    "![68](https://user-images.githubusercontent.com/38183218/43626428-b3f2c78e-972c-11e8-89ed-a67b22d9d86d.png)\n",
    "\n",
    "- repartition\n",
    "    - 데이터셋 재파티션\n",
    "    - 필요할때만 가끔\n",
    "\n",
    "\n",
    "![69](https://user-images.githubusercontent.com/38183218/43626429-b41bc7f6-972c-11e8-8f84-42fec417ae6e.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action\n",
    "\n",
    "![70](https://user-images.githubusercontent.com/38183218/43628874-76d22b4c-9736-11e8-9d06-7f31ed876fad.PNG)\n",
    "\n",
    "_cache하면 반드시 cache메모리 해제해야 _\n",
    "\n",
    "![71](https://user-images.githubusercontent.com/38183218/43628875-76fd1b86-9736-11e8-8a36-1516ae5df476.PNG)\n",
    "\n",
    "- reduce\n",
    "    - 특정 함수를 사용해 RDD의 개수 줄인다\n",
    "    \n",
    "![72](https://user-images.githubusercontent.com/38183218/43628876-7726c4ae-9736-11e8-9768-0f1bcfe4aded.png)\n",
    "\n",
    "- reduceByKey\n",
    "    - 키 값을 기반으로 reduce()\n",
    "![72-1](https://user-images.githubusercontent.com/38183218/43628878-7752dfe4-9736-11e8-82cc-8142616d4596.png)\n",
    "\n",
    "- count\n",
    "    - RDD의 엘리먼트 개수를 샌다\n",
    "    - 전체 데이터셋을 드라이버로 불러오지 않음\n",
    "    \n",
    "- foreach\n",
    "    - RDD의 각 엘리먼트에 반복적으로 적용\n",
    "    - map()과 달리 정의된 함수를 각 데이터에 적용\n",
    "  \n",
    "- saveAsTextFile\n",
    "    - RDD를 텍스트 파일로 저장\n",
    "    - 모든 행이 문자열로 표현\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame\n",
    "- DataFrame\n",
    "    - 내부구조는 RDD이지만 그걸 한번감싸서 구조적으로 관리해줌\n",
    "    - 구조적 데이터 처리 (R, Pandas의 DataFrame처럼)\n",
    "    - 관계형 데이터베이스처럼, 컬럼으로 구성된 분산 데이터 컬렉션\n",
    "    - Immutable(변경 불가), Lazy Evaluations(Action에서 수행), Distributed(분산처리)\n",
    "    - Spark SQL 사용가능 -> RDB처럼 이용가능\n",
    "    - numpy, pandas 등과 연계해서 파이썬과 연계잘됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Catalyst Optimizer\n",
    "    - 논리적 Plan을 컴파일하고 최적화, 가장 효과적인 물리적 Plan을 결정\n",
    "\n",
    "- Tungsten\n",
    "    - 알고리즘 성능 향상(H/W 성능을 최대 활용, Memory, CPU 효율 향상)\n",
    "    - JVM 객체 모델의 오버헤드와 GC(Garbage Collection)가 제거되도록 메모리를 직접적으로 관리\n",
    "\n",
    "----------------------\n",
    "이러한 내부적인 효율성 때문에 어떤 언어쓰든 속도차이 줄어든다\n",
    "![95](https://user-images.githubusercontent.com/38183218/43628881-77aa2150-9736-11e8-95bc-e3ef83858450.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SparkSession 생성\n",
    "session도 SparkContext와 같은 드라이버를 쓴다, 껍데기만 다르고 다 같음\n",
    "\n",
    "![98](https://user-images.githubusercontent.com/38183218/43629849-71cc814e-9739-11e8-9769-3b48ba4912cb.PNG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구조화된 데이터로 불러와(테이블) sql사용한다\n",
    "![101](https://user-images.githubusercontent.com/38183218/43693158-6b055284-9967-11e8-8969-0d86ee4bfc1c.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json형식으로 rdd만들어 df로 읽을수있다 당연히 결과는 같다\n",
    "![104](https://user-images.githubusercontent.com/38183218/43693159-6b33a58a-9967-11e8-9114-f600eefd6c6c.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![105](https://user-images.githubusercontent.com/38183218/43693161-6b603992-9967-11e8-93ee-1cafc87697cf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![107](https://user-images.githubusercontent.com/38183218/43693163-6b8c4bf4-9967-11e8-980c-846ca6c15b22.PNG)\n",
    "![107-2](https://user-images.githubusercontent.com/38183218/43693164-6bc1c306-9967-11e8-994a-a78d03ea5102.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이외에도 rdbms에서 테이블 구조만들고 데이터 입력하듯이\n",
    "\n",
    "데이터와 스키마를 따로 만들고 createDataFrame으로 합치는 방법있다\n",
    "![109](https://user-images.githubusercontent.com/38183218/43693165-6bee8378-9967-11e8-936e-3fd5c72b3712.PNG)\n",
    "![109-2](https://user-images.githubusercontent.com/38183218/43693166-6c28581e-9967-11e8-8c35-3fe2ecaffea7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기호에 따라 sql을 섞어 사용하자, 크게 3가지 형식\n",
    "\n",
    "![111](https://user-images.githubusercontent.com/38183218/43698114-3771debc-9983-11e8-9c3e-22a396be9c3d.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스트리밍\n",
    "![145](https://user-images.githubusercontent.com/38183218/43699323-4a7368b8-9989-11e8-9db6-40076c87ba3a.png)\n",
    "![147](https://user-images.githubusercontent.com/38183218/43699294-2cec1fce-9989-11e8-8f04-4fa37a038cec.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netcat을 이용해 간단히 클라이언트, 서버 환경을 구축하고 스트리밍해본다\n",
    "![150](https://user-images.githubusercontent.com/38183218/43698115-37972ece-9983-11e8-88b9-85a56752b3aa.PNG)\n",
    "![150-2](https://user-images.githubusercontent.com/38183218/43698116-37ba35a4-9983-11e8-955a-0a1147248236.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Structured Streaming\n",
    "    - Advantages of DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![151](https://user-images.githubusercontent.com/38183218/43699295-2d1bfca8-9989-11e8-8efe-99c38b077f88.png)\n",
    "![152](https://user-images.githubusercontent.com/38183218/43699296-2d49c37c-9989-11e8-8caa-49a2248dad75.png)\n",
    "![153](https://user-images.githubusercontent.com/38183218/43699297-2da17acc-9989-11e8-968b-3cf3a2e5b976.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![155-2](https://user-images.githubusercontent.com/38183218/43699324-4b0c21b6-9989-11e8-88bf-1395d34dd6f1.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리\n",
    "중복값, 결측치 처리를 DataFrame에서 해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/43749543-8c434d52-9a2f-11e8-8dad-160927ae1e3d.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749545-8c6e4b2e-9a2f-11e8-850f-b9def35f0690.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749546-8c965bf0-9a2f-11e8-91df-65660b1db635.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749547-8cc0acf2-9a2f-11e8-881e-3736a90ea009.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749548-8cec04ec-9a2f-11e8-8b7c-30afffef89e6.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749549-8d1928e6-9a2f-11e8-9871-7406f352653e.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749550-8d425f7c-9a2f-11e8-834f-963b047ed37e.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이상치 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/38183218/43749552-8d6bd7bc-9a2f-11e8-8ed3-3d9da66db1e3.PNG)\n",
    "![](https://user-images.githubusercontent.com/38183218/43749555-909cfd80-9a2f-11e8-8990-885424a30ce2.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
