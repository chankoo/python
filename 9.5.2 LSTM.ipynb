{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: ratsgo's blog\n",
    "\n",
    "https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, cprev):\n",
    "    xs, hs, cs, is_, fs, os, gs, ys, ps= {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev) # t=0일때 t-1 시점의 hidden state가 필요하므로\n",
    "    cs[-1] = np.copy(cprev)\n",
    "    loss = 0\n",
    "    H = hidden_size\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        tmp = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh  # hidden state\n",
    "        is_[t] = sigmoid(tmp[:H])\n",
    "        fs[t] = sigmoid(tmp[H:2 * H])\n",
    "        os[t] = sigmoid(tmp[2 * H: 3 * H])\n",
    "        gs[t] = np.tanh(tmp[3 * H:])\n",
    "        cs[t] = fs[t] * cs[t-1] + is_[t] * gs[t]\n",
    "        hs[t] = os[t] * np.tanh(cs[t])\n",
    "\n",
    "    # compute loss\n",
    "    for i in range(len(targets)):\n",
    "        idx = len(inputs) - len(targets) + i\n",
    "        ys[idx] = np.dot(Why, hs[idx]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[idx] = np.exp(ys[idx]) / np.sum(np.exp(ys[idx]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[idx][targets[i], 0])  # softmax (cross-entropy loss)\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext, dcnext = np.zeros_like(hs[0]), np.zeros_like(cs[0])\n",
    "    n = 1\n",
    "    a = len(targets) - 1\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        if n > len(targets):\n",
    "            continue\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[a]] -= 1  # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dc = dcnext + (1 - np.tanh(cs[t]) * np.tanh(cs[t])) * dh * os[t]  # backprop through tanh nonlinearity\n",
    "        dcnext = dc * fs[t]\n",
    "        di = dc * gs[t]\n",
    "        df = dc * cs[t-1]\n",
    "        do = dh * np.tanh(cs[t])\n",
    "        dg = dc * is_[t]\n",
    "        ddi = (1 - is_[t]) * is_[t] * di\n",
    "        ddf = (1 - fs[t]) * fs[t] * df\n",
    "        ddo = (1 - os[t]) * os[t] * do\n",
    "        ddg = (1 - np.tanh(gs[t]) * np.tanh(gs[t])) * dg\n",
    "        da = np.hstack((ddi.ravel(),ddf.ravel(),ddo.ravel(),ddg.ravel()))\n",
    "        dWxh += np.dot(da[:,np.newaxis],xs[t].T)\n",
    "        dWhh += np.dot(da[:,np.newaxis],hs[t-1].T)\n",
    "        dbh += da[:, np.newaxis]\n",
    "        dhnext = np.dot(Whh.T, da[:, np.newaxis])\n",
    "        n += 1\n",
    "        a -= 1\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1], cs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
