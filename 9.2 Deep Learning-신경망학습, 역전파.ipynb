{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 학습\n",
    "\n",
    "- 신경망 학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동적으로 획득하는 것\n",
    "- 신경망은 이미지에 포함된 중요한 특징까지도 ‘기계＇가 스스로 학습\n",
    "\n",
    "#### - end-to-end 학습 : 입력 및 출력을 모델이 직접 고려하여 네트워크 가중치를 최적화 !\n",
    "- 모델의 모든 매개변수가 하나의 손실함수에 대해 동시에 훈련되는 경로가 가능한 네트워크로써 역전파 알고리즘 (Backpropagation Algorithm) 과 함께 최적화 될 수 있다는 의미이다. 즉, 신경망은 한쪽 끝에서 입력을 받아들이고 다른 쪽 끝에서 출력을 생성하는데, 입력 및 출력을 직접 고려하여 네트워크 가중치를 최적화 하는 학습을 종단 간 학습(End-to-end Learning) 이라고 한다.\n",
    "- 예를들어 손글씨 인식하는 알고리즘을 생각하기는 쉽지않다. 그렇기에 알고리즘을 설계하고 주어진 데이터의 feature를 추출하는 일을 사람이 아니라 기계가 담당하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 손실함수\n",
    "- 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현 \n",
    "- 그리고 가중치 매개변수의 값을 탐색\n",
    "- 신경망 학습에서 사용하는 지표를 '손실함수(loss function)'\n",
    "- 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차\n",
    "![38](https://user-images.githubusercontent.com/38183218/44009812-575601e8-9ee9-11e8-9fb9-b5a466ffe21d.png)\n",
    "\n",
    "#### 교차 엔트로피 오차\n",
    "![39](https://user-images.githubusercontent.com/38183218/44009813-577c93c6-9ee9-11e8-86a3-b4926b67a600.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_k$ 는 신경망의 출력, \n",
    "\n",
    "$t_k$ 는 정답 레이블(원-핫 인코딩), \n",
    "\n",
    "k는 데이터의 차원수를 나타냅니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5* np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003 0.510825457099338\n"
     ]
    }
   ],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "print(mean_squared_error(np.array(y), np.array(t)),\\\n",
    "cross_entropy_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 손실함수 사용 이유\n",
    "-  왜 ‘정확도’를 사용하지 않고, ‘손실함수’를 사용하는가?\n",
    "     - ‘미분’을 하기 위해서\n",
    "     - 매개 변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신\n",
    "- 정확도\n",
    "    - 0.32, 0.33, 0.34와 같이 불연속적인 값으로 바꿈 \n",
    "        - -> 매개 변수의 미분이 불가\n",
    " - 손실함수\n",
    "     - 0.92543…과 같이 연속적인 수치\n",
    "     - 매개변수가 조금 조정되면 그에 반응하여 손실함수 값도 연속적으로 변화\n",
    "        - -> 매개 변수의 미분 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "모든 변수의 편미분을 벡터로 정리\n",
    "\n",
    "수치미분으로 구현해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1-fxh2)/(2*h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad\n",
    "\n",
    "def func1(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(func1, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "- 손실 함수가 최소값이 될 때의 매개변수값(최적값)을 찾아야한다\n",
    "\n",
    "![46](https://user-images.githubusercontent.com/38183218/44009814-57a05ec8-9ee9-11e8-981d-c526dd7ebefc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(func1, np.array([-3.0,4.0]), lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26d2525fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_descent_plt(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append( x.copy() )\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "lr = 0.1\n",
    "step_num = 20\n",
    "x, x_history = gradient_descent_plt(func1, np.array([-3.0,4.0]), lr=lr, step_num=step_num)\n",
    "\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a= np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a/sum_exp_a\n",
    "    return y\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3780229  -0.76793745 -1.19479352]\n",
      " [-0.59362813  1.36446206  0.72286274]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.30745157  0.76725338 -0.06629964]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5743078394777154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,1,0])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "        return 1 / (1+np.exp(-x))\n",
    "    \n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100,output_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params['W2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100,784)\n",
    "t = np.random.rand(100,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 784 is out of bounds for axis 0 with size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-612eb098789b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-9ee4c66dbd16>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-24c30c69d4e2>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mtmp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_val\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 784 is out of bounds for axis 0 with size 784"
     ]
    }
   ],
   "source": [
    "grads = net.numerical_gradient(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
